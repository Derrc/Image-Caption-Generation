{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Derrc/Image-Caption-Generation/blob/main/image_captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II13sqyRTm8G",
        "outputId": "aeb3d061-37c6-4969-aa4f-3b40fbd30b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/gdrive/MyDrive/ML/Flicker8k/Images.zip, /content/gdrive/MyDrive/ML/Flicker8k/Images.zip.zip or /content/gdrive/MyDrive/ML/Flicker8k/Images.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip /content/gdrive/MyDrive/ML/Flicker8k/Images.zip -d /content/gdrive/MyDrive/ML/Flicker8k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtWNOyjO-_pG",
        "outputId": "cb455973-debc-41ac-9dc6-5d860d362908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet34\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from collections import defaultdict\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "# mount google drive for saving checkpoints\n",
        "drive.mount('/content/gdrive')\n",
        "CDIR = '/content/gdrive/MyDrive/ML'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing**"
      ],
      "metadata": {
        "id": "4xu5g2R0PLIi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ8eJBF_a0YP",
        "outputId": "db595a6b-0173-4b3c-f470-bca8189ad38a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 9865\n"
          ]
        }
      ],
      "source": [
        "# read from caption and image files\n",
        "IMAGE_PATH = CDIR + '/Flickr8k/Images/'\n",
        "LABEL_PATH = CDIR + '/Flickr8k/captions.txt'\n",
        "START_TOKEN, END_TOKEN, PAD_TOKEN = '<start>', '<end>', '<pad>'\n",
        "with open(LABEL_PATH) as f:\n",
        "    lines = f.read().split('\\n')\n",
        "    # first and last lines are not images\n",
        "    lines = [line.split(',', maxsplit=1) for line in lines][1:-1]\n",
        "\n",
        "# dictionary of image_file -> captions\n",
        "captions = defaultdict(list)\n",
        "for line in lines:\n",
        "    image = line[0]\n",
        "    # add start and end tokens\n",
        "    caption = START_TOKEN + ' ' + line[1] + ' ' + END_TOKEN\n",
        "    captions[image].append(caption)\n",
        "\n",
        "# parse caption vocabulary for tokens\n",
        "tokens = set()\n",
        "tokens.add(PAD_TOKEN)\n",
        "for caption in captions.values():\n",
        "    words = [word for sentence in caption for word in sentence.split(' ')]\n",
        "    tokens = tokens.union(set(words))\n",
        "\n",
        "# total number of tokens (vocab size)\n",
        "num_tokens = len(tokens)\n",
        "tokens = list(tokens)\n",
        "tokens_to_id = dict((token, i) for i, token in enumerate(tokens))\n",
        "start_id = tokens_to_id[START_TOKEN]\n",
        "end_id = tokens_to_id[END_TOKEN]\n",
        "\n",
        "print(f'Vocabulary Size: {num_tokens}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VeITP2nSe3Jt"
      },
      "outputs": [],
      "source": [
        "class Flickr8k(Dataset):\n",
        "    def __init__(self, captions, transform=None):\n",
        "        self.data = captions\n",
        "        self.images = list(self.data.keys())\n",
        "        self.annotations = list(self.data.values())\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_file = self.images[index]\n",
        "        annotations = self.annotations[index]\n",
        "        image = Image.open(IMAGE_PATH + image_file)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h15O0KLLHfDI"
      },
      "outputs": [],
      "source": [
        "# Resize images for uniformity in training\n",
        "TARGET_SIZE = (256, 256)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(TARGET_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = Flickr8k(captions, transform=transform)\n",
        "train_test_split = random_split(dataset, [0.9, 0.1])\n",
        "traindata, testdata = train_test_split[0], train_test_split[1]\n",
        "trainloader = DataLoader(traindata, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = DataLoader(testdata, batch_size=1, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decoder and Encoder Networks**"
      ],
      "metadata": {
        "id": "g3Dgj9aqPEo0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YJBZ_YHG_j0V"
      },
      "outputs": [],
      "source": [
        "# ResNet-34 CNN Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, output_dim=14):\n",
        "        super().__init__()\n",
        "        resnet = resnet34(pretrained=True)\n",
        "        layers = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*layers)\n",
        "        # adaptive pool layer so encoder can take images of different sizes\n",
        "        self.resize = nn.AdaptiveAvgPool2d((output_dim, output_dim))\n",
        "\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        x = self.resize(x)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        return x\n",
        "\n",
        "    # disable learning up to first three res blocks\n",
        "    def fine_tune(self):\n",
        "        for l in list(self.resnet.children())[:5]:\n",
        "            for p in l.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "# Soft-Attention Network\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        super().__init__()\n",
        "        # [b_size, image_size, encoder_dim]\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
        "        # [b_size, decoder_dim]\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
        "        self.att = nn.Linear(attention_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    # takes in features from encoder and hidden layer from decoder\n",
        "    def forward(self, features, hidden):\n",
        "        att_features = self.encoder_att(features)\n",
        "        att_hidden = self.decoder_att(hidden)\n",
        "        att_cat = self.relu(att_features + att_hidden.unsqueeze(1))\n",
        "        alpha_logits = self.att(att_cat).squeeze(2)\n",
        "        # [b_size, image_size]\n",
        "        alpha = self.softmax(alpha_logits)\n",
        "        # weighted values for each pixel in feature map\n",
        "        features_weighted = (features * alpha.unsqueeze(2)).sum(dim=1)\n",
        "\n",
        "        return features_weighted, alpha\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, decoder_dim, attention_dim, num_tokens, embed_size, device, encoder_dim=512):\n",
        "        super().__init__()\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.num_tokens = num_tokens\n",
        "        self.device = device\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
        "\n",
        "        self.init_h0 = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.init_c0 = nn.Linear(encoder_dim, decoder_dim)\n",
        "\n",
        "        self.embedding = nn.Embedding(num_tokens, embed_size)\n",
        "        self.lstm = nn.LSTMCell(embed_size + encoder_dim, decoder_dim)\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, num_tokens)\n",
        "        \n",
        "    def initialize(self, features):\n",
        "        # [b_size, image_size, encoder_dim]\n",
        "        features = features.mean(dim=1)\n",
        "        h0 = self.init_h0(features)\n",
        "        c0 = self.init_c0(features)\n",
        "        \n",
        "        return h0, c0\n",
        "\n",
        "    # captions: [b_size, max_length]\n",
        "    def forward(self, features, captions, caption_lengths):\n",
        "        batch_size = features.shape[0]\n",
        "\n",
        "        # [b_size, image_size, encoder_dim]\n",
        "        features = features.reshape(batch_size, -1, self.encoder_dim)\n",
        "\n",
        "        # sort captions and features in descending order by caption length\n",
        "        caption_lengths, sort_indices = caption_lengths.sort(descending=True)\n",
        "        captions = captions[sort_indices]\n",
        "        features = features[sort_indices]\n",
        "\n",
        "        h, c = self.initialize(features)\n",
        "        # [b_size, max_length, embed_size]\n",
        "        embedding = self.embedding(captions)\n",
        "\n",
        "        # exclude end token\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "        max_length = max(decode_lengths)\n",
        "        # storage tensors\n",
        "        logits = torch.zeros(batch_size, max_length, self.num_tokens).to(self.device)\n",
        "        alphas = torch.zeros(batch_size, max_length, features.shape[1]).to(self.device)\n",
        "\n",
        "        for t in range(max_length):\n",
        "            batch_t = sum([l > t for l in decode_lengths])\n",
        "            # [b_size, encoder_dim]\n",
        "            features_weighted, alpha = self.attention(features[:batch_t], h[:batch_t])\n",
        "            # pass weighted features through gate (from paper)\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_t]))\n",
        "            features_weighted = features_weighted * gate\n",
        "\n",
        "            # cat: [b_size, embed_size], [b_size, encoder_dim]\n",
        "            input = torch.cat((embedding[:batch_t, t, :], features_weighted), dim=1)\n",
        "            h, c = self.lstm(input, (h[:batch_t], c[:batch_t]))\n",
        "\n",
        "            logit = self.fc(self.dropout(h))\n",
        "            logits[:batch_t, t, :] = logit\n",
        "            alphas[:batch_t, t, :] = alpha\n",
        "\n",
        "        return logits, alphas, captions, decode_lengths\n",
        "    \n",
        "    # naivce greedy caption generation (BEAM search implemented below)\n",
        "    def generate_caption(self, features, seed_phrase='<start>', max_length=25): \n",
        "        features = features.reshape(1, -1, self.encoder_dim)\n",
        "        embedding = self.embedding(torch.tensor([tokens_to_id[seed_phrase]]).to(self.device))\n",
        "        h, c = self.initialize(features)\n",
        "\n",
        "        output = []\n",
        "        for i in range(max_length):\n",
        "            features_weighted, alpha = self.attention(features, h)\n",
        "            gate = self.sigmoid(self.f_beta(h))\n",
        "            features_weighted = features_weighted * gate\n",
        "\n",
        "            input = torch.cat((embedding, features_weighted), dim=1)\n",
        "            h, c = self.lstm(input, (h, c))\n",
        "\n",
        "            logits = self.fc(h).squeeze(0)\n",
        "            # greedy\n",
        "            next_token = torch.argmax(torch.softmax(logits, dim=0))\n",
        "            # break if end token reached\n",
        "            if next_token == end_id:\n",
        "                break\n",
        " \n",
        "            output.append(tokens[next_token])\n",
        "            embedding = self.embedding(torch.tensor([next_token]).to(self.device))\n",
        "\n",
        "        return ' '.join(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "JdekzPd8PA00"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COqJzgWB9vwo"
      },
      "outputs": [],
      "source": [
        "ENCODER_PATH = CDIR + '/Image-Captioning/encoder.pth'\n",
        "DECODER_PATH = CDIR + '/Image-Captioning/decoder.pth'\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ALPHA_COEF = 1\n",
        "\n",
        "encoder = Encoder().to(DEVICE)\n",
        "decoder = Decoder(256, 256, num_tokens, 300, DEVICE).to(DEVICE)\n",
        "encoder_optim = torch.optim.Adam(encoder.parameters(), lr=1e-4)\n",
        "decoder_optim = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
        "\n",
        "if os.path.exists(ENCODER_PATH):\n",
        "    encoder.load_state_dict(torch.load(ENCODER_PATH))\n",
        "if os.path.exists(DECODER_PATH):\n",
        "    decoder.load_state_dict(torch.load(DECODER_PATH))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7xdl98jJigTG"
      },
      "outputs": [],
      "source": [
        "# return lengths of all sentences in captions (for padding purposes)\n",
        "def max_caption_length(captions):\n",
        "    return max(map(len, [sentence.split(' ') for caption in captions for sentence in caption]))\n",
        " \n",
        " # one-hot encoding of tokens in captions\n",
        "def get_matrix_and_lengths(captions):\n",
        "    max_length = max_caption_length(captions)\n",
        "    # [5, b_size, max_length]\n",
        "    matrix = np.zeros((len(captions), len(captions[0]), max_length), dtype=np.int64) + tokens_to_id[PAD_TOKEN]\n",
        "    lengths = np.zeros((len(captions), len(captions[0])), dtype=np.int64)\n",
        "    for i, caption in enumerate(captions):\n",
        "        for j, sentence in enumerate(caption):\n",
        "            lengths[i][j] = len(sentence.split(' '))\n",
        "            for k, word in enumerate(sentence.split(' ')):\n",
        "                matrix[i][j][k] = tokens_to_id[word]\n",
        "\n",
        "    return matrix, lengths\n",
        "\n",
        "def plot(total_loss):\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.plot(total_loss)\n",
        "    plt.title('Loss')\n",
        "    plt.show()\n",
        "\n",
        "def imshow(image, caption):\n",
        "    plt.title(f'{caption}')\n",
        "    plt.imshow(image.squeeze(0).permute(1, 2, 0).cpu().numpy())\n",
        "\n",
        "def display(encoder, decoder):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    image = next(iter(testloader))[0].to(DEVICE)\n",
        "    features = encoder(image)\n",
        "    caption = decoder.generate_caption(features)\n",
        "\n",
        "    imshow(image, caption)\n",
        "\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "\n",
        "def train(epochs):\n",
        "    total_loss = []\n",
        "    # steps before evaluating/plotting\n",
        "    iters = 5\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    for e in range(epochs):\n",
        "        for i, data in enumerate(trainloader):\n",
        "            images, captions = data\n",
        "            images = images.to(DEVICE)\n",
        "            matrix, lengths = get_matrix_and_lengths(captions)\n",
        "            encoded_captions = torch.tensor(matrix, dtype=torch.int64).to(DEVICE)\n",
        "            caption_lengths = torch.tensor(lengths, dtype=torch.int64).to(DEVICE)\n",
        "            \n",
        "            # mini-mini-batch of each caption\n",
        "            for c in range(len(encoded_captions)):\n",
        "                caption = encoded_captions[c]\n",
        "                caption_length = caption_lengths[c]\n",
        "\n",
        "                features = encoder(images)\n",
        "                logits, alphas, sorted_caption, decode_lengths = decoder(features, caption, caption_length)\n",
        "\n",
        "                # get next tokens and exclude padded timesteps\n",
        "                next_tokens = sorted_caption[:, 1:]\n",
        "                next_tokens = pack_padded_sequence(next_tokens, decode_lengths, batch_first=True)[0]\n",
        "                logits = pack_padded_sequence(logits, decode_lengths, batch_first=True)[0]\n",
        "\n",
        "                loss = criterion(logits, next_tokens)\n",
        "                # alpha regularization as shown in paper\n",
        "                loss += ALPHA_COEF * ((1 - alphas.sum(dim=1)).pow(2)).mean()\n",
        "\n",
        "                encoder_optim.zero_grad()\n",
        "                decoder_optim.zero_grad()\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                encoder_optim.step()\n",
        "                decoder_optim.step()\n",
        "\n",
        "                total_loss.append(loss.detach().cpu().numpy())\n",
        "\n",
        "            if (i+1) % iters == 0:\n",
        "                print(f'[{e+1}, {i+1}] Loss: {np.mean(total_loss[-iters]):.3f}')\n",
        "                plot(total_loss)\n",
        "                display(encoder, decoder)\n",
        "\n",
        "                torch.save(encoder.state_dict(), ENCODER_PATH)\n",
        "                torch.save(decoder.state_dict(), DECODER_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65WxRNE4oAye"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "train(epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Beam Search**"
      ],
      "metadata": {
        "id": "sySNVzBOOu-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# caption given image\n",
        "def beam_search(encoder, decoder, image, k=3, max_length=35):\n",
        "    # image preprocessing\n",
        "\n",
        "    features = encoder(image)\n",
        "    # [1, num_pixels, 512]\n",
        "    features = features.reshape(1, -1, features.shape[3])\n",
        "    # initialize with batch of k (considering top k candidates for first step)\n",
        "    features = features.expand(k, features.shape[1], features.shape[2])\n",
        "\n",
        "    # storage tensors: [k, 1]\n",
        "    k_prev_words = torch.tensor([[start_id]] * k, dtype=torch.int64).to(DEVICE)\n",
        "    k_seq = torch.tensor([[start_id]] * k, dtype=torch.int64).to(DEVICE)\n",
        "    top_k_scores = torch.zeros(k, 1).to(DEVICE)\n",
        "\n",
        "    # storage lists\n",
        "    seqs_done = list()\n",
        "    seqs_done_scores = list()\n",
        "\n",
        "    step = 1\n",
        "    h, c = decoder.initialize(features)\n",
        "    for step in range(max_length):\n",
        "        # [k, embed_size]\n",
        "        embedding = decoder.embedding(k_prev_words.squeeze(1))\n",
        "\n",
        "        features_weighted, alpha = decoder.attention(features, h)\n",
        "        gate = torch.sigmoid(decoder.f_beta(h))\n",
        "        # [k, encoder_dim]\n",
        "        features_weighted = features_weighted * gate\n",
        "\n",
        "        input = torch.cat((embedding, features_weighted), dim=1)\n",
        "        h, c = decoder.lstm(input, (h, c))\n",
        "        # [k, num_tokens]\n",
        "        logits = decoder.fc(h)\n",
        "        # generate all possible [step, step+1] scores, pick top k\n",
        "        scores = torch.log_softmax(logits, dim=1)\n",
        "\n",
        "        # scores = sum of log probs\n",
        "        scores = top_k_scores.expand_as(scores) + scores\n",
        "        top_k_scores, top_k_tokens = scores.flatten().topk(k, 0, True, True)\n",
        "\n",
        "        # gets which previous k seq the tokens are part of\n",
        "        prev_k_indices = top_k_tokens / num_tokens\n",
        "        prev_k_indices = prev_k_indices.long()\n",
        "        # gets token_ids\n",
        "        next_k_indices = top_k_tokens % num_tokens\n",
        "\n",
        "        # add to previous sequences\n",
        "        k_seq = torch.cat((k_seq[prev_k_indices], next_k_indices.unsqueeze(1)), dim=1)\n",
        "\n",
        "        # find all indices that reached <end>\n",
        "        indices_not_done = [i for i, token in enumerate(next_k_indices) if token != end_id]\n",
        "        indices_done = list(set(range(len(next_k_indices))) - set(indices_not_done))\n",
        "\n",
        "        if len(indices_done) > 0:\n",
        "            seqs_done.extend(k_seq[indices_done].tolist())\n",
        "            seqs_done_scores.extend(top_k_scores[indices_done].tolist())\n",
        "            k -= len(indices_done)\n",
        "\n",
        "        # break if all sequences are terminated\n",
        "        if k == 0:\n",
        "            break\n",
        "        \n",
        "        # update variables to continue from chosen prev sequences\n",
        "        prev_seq = prev_k_indices[indices_not_done]\n",
        "        k_seq = k_seq[indices_not_done]\n",
        "        h = h[prev_seq]\n",
        "        c = c[prev_seq]\n",
        "        features = features[prev_seq]\n",
        "        top_k_scores = top_k_scores[indices_not_done].unsqueeze(1)\n",
        "        k_prev_words = next_k_indices[indices_not_done].unsqueeze(1)\n",
        "\n",
        "    \n",
        "    ind = np.argmax(seqs_done_scores)\n",
        "    best_seq = seqs_done[ind]\n",
        "\n",
        "    best_seq = [tokens[id] for id in best_seq]\n",
        "\n",
        "    return ' '.join(best_seq)"
      ],
      "metadata": {
        "id": "kE04Z_747OTA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate Captions From Test Dataset**"
      ],
      "metadata": {
        "id": "0gPxOmfyNXBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "image = next(iter(testloader))[0].to(DEVICE)\n",
        "features = encoder(image)\n",
        "caption = beam_search(encoder, decoder, image)\n",
        "\n",
        "imshow(image, caption)"
      ],
      "metadata": {
        "id": "JRCeRNwjOlS-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/5YMbwPclu1ta14Y2M8bu",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}